training_infos:
{
    "total_epochs": 140,
    "trainset_infos": {
        "number_of_chars": 49,
        "length_of_sequence": 5,
        "padding_end": "*",
        "target_group": "Hobbit",
        "m": 3920,
        "padding_start": "#"
    },
    "acc": 0.875,
    "loss": 0.302446913932051
}
history["hyperparams"]:
[
    [
        0,
        {
            "batch_size": 32,
            "lr": 0.003,
            "loss": "categorical_crossentropy"
        }
    ],
    [
        100,
        {
            "batch_size": 32,
            "lr": 0.0003,
            "loss": "categorical_crossentropy"
        }
    ],
    [
        120,
        {
            "batch_size": 32,
            "lr": 0.0001,
            "loss": "categorical_crossentropy"
        }
    ]
]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         (None, 5, 49)             0         
_________________________________________________________________
lstm_6 (LSTM)                (None, 64)                29184     
_________________________________________________________________
dense_6 (Dense)              (None, 49)                3185      
_________________________________________________________________
activation_6 (Activation)    (None, 49)                0         
=================================================================
Total params: 32,369
Trainable params: 32,369
Non-trainable params: 0
_________________________________________________________________
