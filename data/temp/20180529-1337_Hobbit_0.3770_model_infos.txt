training_infos:
{
    "total_epochs": 200,
    "trainset_infos": {
        "number_of_chars": 49,
        "length_of_sequence": 5,
        "padding_end": "*",
        "target_group": "Hobbit",
        "m": 3920,
        "padding_start": "#"
    },
    "acc": 0.864030612244898,
    "loss": 0.37704601233102836
}
history["hyperparams"]:
[
    [
        0,
        {
            "batch_size": 32,
            "lr": 0.0003,
            "loss": "categorical_crossentropy"
        }
    ]
]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_4 (InputLayer)         (None, 5, 49)             0         
_________________________________________________________________
lstm_4 (LSTM)                (None, 128)               91136     
_________________________________________________________________
dense_4 (Dense)              (None, 49)                6321      
_________________________________________________________________
activation_4 (Activation)    (None, 49)                0         
=================================================================
Total params: 97,457
Trainable params: 97,457
Non-trainable params: 0
_________________________________________________________________
