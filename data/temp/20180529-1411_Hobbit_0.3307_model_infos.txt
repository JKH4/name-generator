training_infos:
{
    "total_epochs": 500,
    "trainset_infos": {
        "number_of_chars": 49,
        "length_of_sequence": 5,
        "padding_end": "*",
        "target_group": "Hobbit",
        "m": 3920,
        "padding_start": "#"
    },
    "acc": 0.8709183673469387,
    "loss": 0.3306666409178656
}
history["hyperparams"]:
[
    [
        0,
        {
            "batch_size": 32,
            "lr": 0.0003,
            "loss": "categorical_crossentropy"
        }
    ]
]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_5 (InputLayer)         (None, 5, 49)             0         
_________________________________________________________________
lstm_5 (LSTM)                (None, 64)                29184     
_________________________________________________________________
dense_5 (Dense)              (None, 49)                3185      
_________________________________________________________________
activation_5 (Activation)    (None, 49)                0         
=================================================================
Total params: 32,369
Trainable params: 32,369
Non-trainable params: 0
_________________________________________________________________
