training_infos:
{
    "total_epochs": 100,
    "trainset_infos": {
        "number_of_chars": 49,
        "length_of_sequence": 5,
        "padding_end": "*",
        "target_group": "Hobbit",
        "m": 3920,
        "padding_start": "#"
    },
    "acc": 0.8543367346938775,
    "loss": 0.4431156362806048
}
history["hyperparams"]:
[
    [
        0,
        {
            "batch_size": 32,
            "lr": 0.0003,
            "loss": "categorical_crossentropy"
        }
    ]
]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         (None, 5, 49)             0         
_________________________________________________________________
lstm_3 (LSTM)                (None, 256)               313344    
_________________________________________________________________
dense_3 (Dense)              (None, 49)                12593     
_________________________________________________________________
activation_3 (Activation)    (None, 49)                0         
=================================================================
Total params: 325,937
Trainable params: 325,937
Non-trainable params: 0
_________________________________________________________________
