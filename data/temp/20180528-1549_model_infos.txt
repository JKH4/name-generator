training_infos:
{
    "total_epochs": 200,
    "loss": 0.34663623911993846,
    "acc": 0.8650510204081633,
    "trainset_infos": {
        "target_race": "Hobbit",
        "length_of_sequence": 5,
        "number_of_chars": 49,
        "m": 3920
    }
}
history["hyperparams"]:
[
    [
        0,
        {
            "lr": 0.0003,
            "loss": "categorical_crossentropy",
            "batch_size": 32
        }
    ]
]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         (None, 5, 63)             0         
_________________________________________________________________
lstm_3 (LSTM)                (None, 256)               327680    
_________________________________________________________________
dense_3 (Dense)              (None, 63)                16191     
_________________________________________________________________
activation_3 (Activation)    (None, 63)                0         
=================================================================
Total params: 343,871
Trainable params: 343,871
Non-trainable params: 0
_________________________________________________________________
